# 强化学习基础

## 今日学习内容

- MC basic
- MC exploring start
- MC $\epsilon$-greedy

## 我自己的理解

- why
  有模型是指，对于一个环境，我们知道他的状态转移概率和奖励函数，然后我们可以根据贝尔曼公式来得到每一个状态值和动作值，然后根据状态值和动作值来进行下一步行动。
  无模型是指，对于一个环境，我们不知道他的状态转移概率和奖励函数，自然也就无法通过贝尔曼公式来得到每一个状态值和动作值，所以我们需要一种方法来计算状态值和动作值。
- what
  蒙特卡洛方法是指，对于一个无模型的环境，可以用经验的方法来计算状态值和动作值。
- how
  分为PE(策略评估)和PI(策略提升)，首先对于一个起始的策略，我们要对其进行策略评估，就要计算他的状态值，我们就可以用蒙特卡洛方法来计算。具体做法是，在PE中，有一个固定的策略 $\pi$，我们要计算状态值或者是动作值，就要用很多个episode中某个状态的回报的平均值来
  近似他的状态值或者是动作值。 在PI中，就可以用 $\pi(a \mid s)$ = argmax a(Q(s,a))来进行策略提升。
- exploring start & $\epsilon$-greedy
   这两个的区别是，前者是随机选择起始点，后者是进行PI的时候，有概率随机选择和贪心算法不同的动作。
  
  
  

